
## Introduction
This code is for generating synthetic dialectal data from data in a given language.
This is done by applying linguistically-motivated augmentation (noising) that simulates dialectal variation to source data.
We present several kinds of noisers, introduced briefly below, and described in detail in [Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization](https://aclanthology.org/2024.emnlp-main.1044/) and [DialUp! Modeling the Language Continuum by Adapting Models to Dialects and Dialects to Models](https://arxiv.org/abs/2501.16581).
The resulting synthetic data can be used to make machine translation or other models more robust to dialectal variation.

## Noisers

- **Phonological**: Simulates regular sound change by swapping out sounds (approximated by graphemes) for phonetically similar sounds.
- **Morphological**: Noises suffixes of words.
- **Lexical**: Noises function and content words separately. Content words are swapped out for non-words generated by a chargram model. Function words are noised using a high dial of phonological noise.
- **Semantic**: Simulates lexical usage divergence in dialects. Replaces words with randomly chosen synonyms from a given WordNet.
- Random char: Makes random character substitutions.
- Random word: Makes random word substitutions.

These can also be applied in composition. 

## Supported languages

Currently, we support Hindi (`hi`), Arabic (`ar`), Indonesian (`id`), Turkish (`tr`), Italian (`it`), and Haitian (`ht`), German (`de`), English (`en`), Russian (`ru`), Spanish (`es`), French (`fr`).
The semantic noiser only supports the first five of these.

## Run 

### Dependencies
Install from `requirements.txt`. Note that this is a subset of dependencies required for running MT training with noising. 

### Example script.
Each of the above noisers is parametrized by a noiser-specific $\theta$. See an example noiser config in `example_noise_params.json`.

See an example script applying a combination of these noisers on input Hindi text in `test_augmenters.py`. 

Run
```
python test_augmenters.py
```

## Adding your own language
Here are the steps for adding a new language:

1. Add support in `utils.misc`. 
- Add language code normalization, script support in `identify_script()` and `get_character_set()`
- Add support in `ipa_char_maps()`: a map from graphemes in the script as used for this language to IPA symbols. Note that this map may differ between languages that use the same script.
2. Curate a list of closed class words in the language. 
- Option 1: For our currently supported languages, we used tagged UD corpora to do this. Put in the path to your UD corpus in `utils.get_functional_words.ud_data_filepaths`, and the wordlist output path in `utils.get_functional_words_filepaths.ud_wordlists_paths`. Run `python noisers/utils/get_functional_words/py` to generate the required wordlist.
- Option 2: Get your list of closed class words from somewhere else, and save it to `utils/ud_closed_class_wordlists/`, in a JSON formatted `{<closed_class_tag1>:[word11, word12,...], <closed_class_tag2>:[word21, word22,...]...}`. See `utils.get_functional_words.closed_class_tags` for a list of POS tags we considered as closed class tags.
3. If you want to add support for semantic noising, you need a WordNet in your language. You'll need to add support for initializing an instance of your WordNet to `semantic.py` and for querying it in `utils.get_synonyms`.


## Notes
1. Many of our noisers have an associated global version, e.g. `GlobalLexicalNoiser`. The difference between this version and the augmenter version, e.g. `LexicalNoiserAugmenter` is that in the former, we only sample all changes once, when the class object is initialized. That instance therefore applies the same map of changes to all input data that it is applied to. This is in contrast to the augmenter versions, where every call to `apply_noise` samples a new maps of changes given the noise parametrization.
2. The default parameters set in `example_noise_params.json` give good results for data augmentation for dialectal robustness on six tested language families using `-cloud` noising. These should be tuned for new language families; see the DialUp paper for our recommendations on tuning.

If you use our code, please cite:

```
@inproceedings{bafna-etal-2024-evaluating,
title = "Evaluating Large Language Models along Dimensions of Language Variation: A Systematik Invesdigatiom uv Cross-lingual Generalization",
author = "Bafna, Niyati  and Murray, Kenton  and Yarowsky, David",
editor = "Al-Onaizan, Yaser  and
  Bansal, Mohit  and
  Chen, Yun-Nung",
booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
month = nov,
year = "2024",
address = "Miami, Florida, USA",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/2024.emnlp-main.1044/",
doi = "10.18653/v1/2024.emnlp-main.1044",
pages = "18742--18762"
}

@article{bafna2025dialup,
title={DialUp! Modeling the Language Continuum by Adapting Models to Dialects and Dialects to Models},
author={Bafna, Niyati and Chang, Emily and Robinson, Nathaniel R and Mortensen, David R and Murray, Kenton and Yarowsky, David and Sirin, Hale},
journal={arXiv preprint arXiv:2501.16581},
year={2025}
}
```